
---

# ğŸ’¬ Ollama AI Prompt Engineering Chatbot

An interactive web-based chatbot built using **React (frontend)** and **Node.js with Express (backend)** that connects to a **local or cloud Ollama model** through the LangChain `ChatOllama` interface.
This chatbot allows users to experiment with **custom system prompts (templates)** and chat dynamically with an AI specialized.

---

## ğŸš€ Features

* ğŸ§  **AI-Powered Responses** â€” Uses Ollama models via LangChain for intelligent, context-aware chat.
* ğŸ§© **Custom Prompt Templates** â€” Users can define their own AI behavior dynamically.
* ğŸ’¬ **Interactive Chat Interface** â€” Clean and responsive UI built with React.
* ğŸ”„ **Auto-Scroll to Latest Message** â€” Chat view automatically focuses on the newest messages.
* âš™ï¸ **Full Express API Backend** â€” Simple REST endpoint for handling AI requests.
* ğŸŒ **CORS-Enabled Communication** â€” Frontend and backend can run independently during development.

---

## ğŸ—ï¸ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js              # Express + LangChain + Ollama logic
â”‚   â””â”€â”€ .env                   # Environment variables (API keys, etc.)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js             # Main React component
â”‚   â”‚   â”œâ”€â”€ Chatbot.css        # Styling for chat interface
â”‚   â”‚   â””â”€â”€ index.js           # React entry point
â”‚   â””â”€â”€ package.json           # Frontend dependencies
â”‚
â””â”€â”€ README.md
```

---

## âš¡ Setup and Run

### 1. Backend (Node.js + Express)

1. Navigate to the backend folder
2. Install dependencies:

   ```bash
   npm install
   ```
3. Start the server:

   ```bash
   node server.js
   ```
4. The API will run on `http://localhost:5000/api/chat`

### 2. Frontend (React)

1. Navigate to the frontend folder
2. Install dependencies:

   ```bash
   npm install
   ```
3. Start the React development server:

   ```bash
   npm start
   ```
4. Access the app at `http://localhost:3000`

---

## ğŸ§© How It Works

* The frontend collects user input and an optional **prompt template**.
* On send, it makes a POST request to `/api/chat` with the message and template.
* The backend uses **LangChainâ€™s ChatOllama** to generate a response from the selected model.
* The chat window displays both user and AI messages with smooth scrolling.

---

## ğŸ–Œï¸ UI Overview

* **System Prompt Area:** Customize how the AI behaves (e.g., tone, domain, language).
* **Chat Area:** Displays conversation history with the AI.
* **Input Field:** Type your questions and send them instantly.
* **Auto Scroll:** The chat view always focuses on the latest response.

---

## ğŸ§  Technologies Used

* **Frontend:** React, CSS
* **Backend:** Node.js, Express, LangChain
* **Model:** Ollama (local or cloud-hosted)
* **Environment Management:** dotenv
* **Cross-Origin Support:** CORS

---

## ğŸ“¦ Requirements

* Node.js 18+
* Ollama installed locally *(or access to a remote Ollama model)*
* Internet access (for cloud models)

---

## ğŸ”’ Environment Variables

Create a `.env` file in your backend directory:

```
OLLAMA_URL=http://localhost:11434
```



---

## ğŸ§­ Future Improvements

* Add **conversation history persistence** (local storage or database)
* Support **multiple AI models** and dynamic selection
* Integrate **Markdown rendering** for AI responses
* Enable **speech input/output**

---

## ğŸ§‘â€ğŸ’» Author

**Budi Setiawan**
Passionate about AI, and application development.


---

